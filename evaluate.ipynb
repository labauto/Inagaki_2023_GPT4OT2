{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# suppress pandas future warning\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# run opentrons_simulate in the terminal and get the stdout\n",
    "# and save it to a file\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "import shutil\n",
    "import datetime\n",
    "import glob\n",
    "from typing import List, Tuple, Dict, Set, Optional, Union, Any, Callable, TypeVar, Generic, Type, cast, overload\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from utils import init_df_eval, save_prompt_and_answer_with_modelname, extract_markdown_code_blocks, prepare_python_script, run_opentrons_simulate, check_filename_extract_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./question_and_answer\"\n",
    "df_eval = init_df_eval(dataset_path)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \"\"\"\n",
    "    evaluate the answer text stored in `./question_and_answer/*.txt`\n",
    "    run `opentrons_simulate` and save the result to the pandas dataframe\n",
    "    \"\"\"\n",
    "    for row in df_eval.itertuples():\n",
    "        file_path = row.prompt_answer_file_path\n",
    "        filename = os.path.basename(file_path)\n",
    "        model, finetuned, prompt_ver = check_filename_extract_model_info(filename)\n",
    "        python_script_path = prepare_python_script(file_path)\n",
    "        output_path, output_text, result_type = run_opentrons_simulate(python_script_path, 'eva.txt')\n",
    "        print(f'prompt_ver: {prompt_ver}, model: {model}, result_type: {result_type}')\n",
    "        df_eval.at[row.Index, 'opentrons_simulate_result_file_path'] = output_path\n",
    "        df_eval.at[row.Index, 'opentrons_simulate_result_raw_text'] = output_text\n",
    "        df_eval.at[row.Index, 'opentrons_simulate_result_last_line'] = output_text.splitlines()[-1]\n",
    "        df_eval.at[row.Index, 'opentrons_simulate_result'] = result_type\n",
    "        df_eval.at[row.Index, 'model'] = model\n",
    "        df_eval.at[row.Index, 'finetuned'] = finetuned\n",
    "        df_eval.at[row.Index, 'prompt_ver'] = prompt_ver\n",
    "        df_eval.at[row.Index, 'python_script_file_path'] = python_script_path\n",
    "    # save df_eval\n",
    "    df_eval.to_csv(dataset_path + '/df_eval.csv', index=False)\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "MODEL_LIST = [ 'ada', 'finetuned-davinci', 'code-davinci-002', 'text-davinci-003',\n",
    "       'chatgpt', 'gpt-3.5-turbo', 'gpt-4', 'gpt-3.5-turbo_1', 'gpt-3.5-turbo_2', 'gpt-3.5-turbo_3',\n",
    "       'gpt-3.5-turbo_4', 'gpt-3.5-turbo_5','gpt-4_1', 'gpt-4_2', 'gpt-4_3', 'gpt-4_4', 'gpt-4_5']\n",
    "\n",
    "def add_error_kind(df_eval: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extract exceptions from `opentrons_simulate_result_raw_text` or `opentrons_simulate_result_last_line` column and add it to `error_kind` column\n",
    "    # e.g. IndentaionError, NameError, etc.\n",
    "    \"\"\"\n",
    "    # extract before ':'\n",
    "    df_eval['error_kind'] = df_eval['opentrons_simulate_result_last_line'].str.extract(r'([a-zA-Z]+):')\n",
    "    # if ExceptionInProtocolError, set error_kind to 'ExceptionInProtocolError'\n",
    "    df_eval.loc[df_eval['opentrons_simulate_result_raw_text'].str.contains('ExceptionInProtocolError'), 'error_kind'] = 'ExceptionInProtocolError'\n",
    "    # if ApiDeprecationError, set error_kind to 'ApiDeprecationError'\n",
    "    df_eval.loc[df_eval['opentrons_simulate_result_raw_text'].str.contains('ApiDeprecationError'), 'error_kind'] = 'ApiDeprecationError'\n",
    "    # MalformedProtocolError\n",
    "    df_eval.loc[df_eval['opentrons_simulate_result_raw_text'].str.contains('MalformedProtocolError'), 'error_kind'] = 'MalformedProtocolError'\n",
    "    # FileNotFoundError\n",
    "    df_eval.loc[df_eval['opentrons_simulate_result_raw_text'].str.contains('FileNotFoundError'), 'error_kind'] = 'FileNotFoundError'\n",
    "    # if error and opentrons_simulate_result_last_line is empty, add UnknownError\n",
    "    df_eval.loc[(df_eval['opentrons_simulate_result'] == 'error') & ((df_eval['error_kind'] == '') | df_eval['error_kind'].isnull()), 'error_kind'] = 'UnknownError'\n",
    "    # if opentrons_simulate_result == 'ok', set error_kind to 'success'\n",
    "    df_eval.loc[df_eval['opentrons_simulate_result'] == 'ok', 'error_kind'] = 'success'\n",
    "    return df_eval\n",
    "\n",
    "def filter_prompt_ver(df_eval: pd.DataFrame, prompt_ver: List[str] = ['v1', 'v2']):\n",
    "    return df_eval[df_eval['prompt_ver'].isin(prompt_ver)]\n",
    "\n",
    "def plot_result(df_eval: pd.DataFrame, prompt_ver: str = 'v1'):\n",
    "    df_eval = df_eval[df_eval['prompt_ver'] == prompt_ver]\n",
    "    # sort by model name\n",
    "    df_eval = df_eval.sort_values(by='model', key=lambda x: x.map(MODEL_LIST.index))\n",
    "    # filter out 'chatgpt\n",
    "    df_eval = df_eval[df_eval['model'] != 'chatgpt']\n",
    "    if len(df_eval) == 0:\n",
    "        print(f'no data for prompt_ver={prompt_ver} and model={model}')\n",
    "        return\n",
    "    # plot\n",
    "\n",
    "    base_palette = sns.color_palette(\"flare\", len(ERROR_LIST))\n",
    "    # color_palette = [base_palette[i] if key == 'success' else 'lightgreen' for i, key  in enumerate(ERROR_LIST) ]\n",
    "    color_palette = {key: base_palette[i] if key != 'success' else 'lightgreen' for i, key  in enumerate(ERROR_LIST) }\n",
    "    # {key: 'green' if key == 'success' else 'red' for key  in error_kind_list }\n",
    "    # fontsize\n",
    "    plt.rcParams['font.size'] = 18\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    sns.countplot(x='model', hue='error_kind', data=df_eval, palette=color_palette)\n",
    "    plt.title(f'result, prompt_ver={prompt_ver}')\n",
    "    # rotate x labe not to overlap\n",
    "    plt.xlabel(None)\n",
    "    plt.xticks(rotation=60)\n",
    "    # plt.show()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./fig/result_prompt_ver_{prompt_ver}.pdf', dpi=300)\n",
    "\n",
    "    # plot table\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    sns.heatmap(pd.crosstab(df_eval['model'], df_eval['error_kind']), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.savefig(f'./fig/result_prompt_ver_{prompt_ver}_table.png', dpi=300)\n",
    "    plt.title(f'result, prompt_ver={prompt_ver}')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_VAR_LIST = ['v1', 'v2', 'v3', 'v4', 'v3.1', 'v4.1']\n",
    "UNIQUE_MODEL_LIST = [\"gpt-4\", \"gpt-3.5-turbo\", \"ada\", \"text-davinci-003\"]\n",
    "\n",
    "ERROR_LIST = ['ApiDeprecationError', 'UnknownError', 'IndentationError', 'RuntimeError', 'SyntaxError', 'ExceptionInProtocolError', None, 'AttributeError', 'success', 'models', 'NameError', 'TabError', 'ValueError', 'MalformedProtocolError', 'FileNotFoundError', 'TypeError']\n",
    "\n",
    "USE_MODEL_LIST = [\"gpt-4_1\", \"gpt-4_2\", \"gpt-4_3\", \"gpt-4_4\", \"gpt-4_5\", \"gpt-3.5-turbo_1\", \"gpt-3.5-turbo_2\", \"gpt-3.5-turbo_3\", \"gpt-3.5-turbo_4\", \"gpt-3.5-turbo_5\"]\n",
    "USE_MODEL_GROUP_LIST = [\"gpt-4\", \"gpt-3.5-turbo\"]\n",
    "USE_COLUMNS = ['prompt_ver', 'model_group', 'error_kind', 'iteration', 'conversation_id']\n",
    "\n",
    "# filter prompt version\n",
    "# USE_PROMPT_VER = ['v1', 'v2', 'v3', 'v4', 'v3.1', 'v4.1', 'v2.cell2', 'v2.medium2', 'v2.cell2.medium2', 'v2-B']\n",
    "# df_eval = filter_prompt_ver(df_eval, prompt_ver=USE_PROMPT_VER)\n",
    "\n",
    "plot_result(add_error_kind(df_eval), prompt_ver='v1')\n",
    "plot_result(add_error_kind(df_eval), prompt_ver='v2')\n",
    "\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2.cell2')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2.medium2')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2.cell2.medium2')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2-B')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2-B.cell2')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2-B.target2')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v2-B.cell2.target2')\n",
    "\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v3')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v4')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v3.1')\n",
    "# plot_result(add_error_kind(df_eval), prompt_ver='v4.1')\n",
    "\n",
    "df_eval = add_error_kind(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_eval.error_kind.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conversation_id(df_eval: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add conversation_id to each row. This is generated inside `call_evaluate_loop` function and is used to identify the conversation.\n",
    "\n",
    "    treat some conversational models as one model, for example, gpt-4_1, gpt-4_2, gpt-4_3, gpt-4_4, gpt-4_5 as gpt-4\n",
    "    but extract the number of the last underscore, for example, gpt-4_5 -> 5\n",
    "\n",
    "    E.g.\n",
    "        | model (str) | prompt_ver (str) | success | error_kind | model_group (str) | count (int) | conversation_id (uuid)\n",
    "        | --- | --- | --- | --- | --- | --- | --- |\n",
    "        | gpt-4_1 | v1 | 0 | 0 | gpt-4 | 1 | uuid |\n",
    "        | gpt-4_2 | v1 | 0 | 0 | gpt-4 | 2 | uuid |\n",
    "        | gpt-4_3 | v1 | 0 | 0 | gpt-4 | 3 | uuid |\n",
    "    \"\"\"\n",
    "    for row in df_eval.itertuples():\n",
    "        # extract the number of the last underscore\n",
    "        count = row.model.split('_')[-1]\n",
    "        # if count is digit, treat the model as one model\n",
    "        if count.isdigit():\n",
    "            df_eval.loc[row.Index, 'model_group'] = row.model.replace(f'_{count}', '')\n",
    "            df_eval.loc[row.Index, 'iteration'] = int(count)\n",
    "            # extract uuid from \n",
    "            filename = df_eval.loc[row.Index, 'prompt_answer_file_path'].split('/')[-1] # chat_loop_1_2c2cef27-69dc-401c-a8af-01bbb295e342_gpt-4_token_100_temperature_0.9_2023-03-27-14-46-47_prompt_v4\n",
    "            df_eval.loc[row.Index, 'conversation_id'] = filename.split('_')[3]\n",
    "            if filename.split('_')[3] == 'last':\n",
    "                df_eval.loc[row.Index, 'conversation_id'] = filename.split('_')[4]\n",
    "        else:\n",
    "            df_eval.loc[row.Index, 'model_group'] = row.model\n",
    "            df_eval.loc[row.Index, 'iteration'] = 0\n",
    "            df_eval.loc[row.Index, 'conversation_id'] = ''\n",
    "    return df_eval\n",
    "\n",
    "df_eval_conversation = add_conversation_id(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_some_columns(df_eval: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Just ignore some columns that are not needed for plotting or table.\n",
    "    \"\"\"\n",
    "    df = df_eval[USE_COLUMNS]\n",
    "    # filter by USE_MODEL_LIST\n",
    "    df = df[df['model_group'].isin(USE_MODEL_GROUP_LIST)]\n",
    "    # iteration to int\n",
    "    df['iteration'] = df['iteration'].astype(int)\n",
    "    # filer iteration zero\n",
    "    df = df[df['iteration'] != 0]\n",
    "    return df\n",
    "\n",
    "df_eval_short = use_some_columns(df_eval_conversation)\n",
    "df_eval_short.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_dataframe(df: pd.DataFrame, model: str, prompt_ver: str) -> Tuple[pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    To create raw data table for the paper.\n",
    "    \"\"\"\n",
    "    # use gpt-4 only\n",
    "    df = df[df['model_group'] == model]\n",
    "    # use v1 only\n",
    "    df = df[df['prompt_ver'] == prompt_ver]\n",
    "    grouped_db = df.groupby(['conversation_id', 'iteration']).error_kind.apply(list).reset_index()\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['Model', 'Prompt', 'Sample No.', 'conversation_id', '1', '2', '3', '4', '5'])\n",
    "    result_df.loc[0] = ['Model', 'Prompt', 'Sample No.', 'conversation_id',  '1', '2', '3', '4', '5']\n",
    "\n",
    "    results = []\n",
    "    sample_no = 1\n",
    "    current_conversation_id = grouped_db['conversation_id'][0]\n",
    "    print(f'unique conversation id length: {len(grouped_db[\"conversation_id\"].unique())}')\n",
    "    for index, row in grouped_db.iterrows():\n",
    "        if row['conversation_id'] != current_conversation_id:\n",
    "            current_conversation_id = row['conversation_id']\n",
    "            sample_no += 1\n",
    "\n",
    "        row_data = [f'{model}', f'{prompt_ver}', str(sample_no), current_conversation_id]\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            if row['iteration'] == i:\n",
    "                row_data.append(row['error_kind'])\n",
    "            else:\n",
    "                row_data.append('')\n",
    "\n",
    "        # append row\n",
    "        result_df.loc[len(result_df)] = [*row_data]\n",
    "        # result_df.loc[sample_no] = [*row_data]\n",
    "        results.append(row_data)\n",
    "\n",
    "    df_grouped = result_df.groupby(['Sample No.', 'Model', 'Prompt', 'conversation_id']).agg({str(i): lambda x: x.dropna().tolist() for i in range(1, 6)})\n",
    "    result_df = result_df[['Sample No.', 'Model', 'Prompt', 'conversation_id']].drop_duplicates().merge(df_grouped.reset_index(), on='Sample No.')\n",
    "    # drop ['Model_y', 'Prompt_y'] and  then rename ['Model_x', 'Prompt_x'] to ['Model', 'Prompt']\n",
    "    result_df = result_df.drop(['Model_y', 'Prompt_y'], axis=1).rename(columns={'Model_x': 'Model', 'Prompt_x': 'Prompt'})\n",
    "    # for [1, 2, 3, 4, 5] columns, convert list to string\n",
    "    for i in range(1, 6):\n",
    "        result_df[str(i)] = result_df[str(i)].apply(lambda x: str(x).replace('[', '').replace(']', '').replace(\"'\", ''))\n",
    "        # also, replace ', ' to ''\n",
    "        result_df[str(i)] = result_df[str(i)].apply(lambda x: x.replace(', ', ''))\n",
    "    # drop first row\n",
    "    result_df = result_df.drop(result_df.index[0])\n",
    "\n",
    "    # save csv\n",
    "    result_df.to_csv(f'./table/all_{model}_{prompt_ver}.csv', index=False)\n",
    "    return result_df, results\n",
    "\n",
    "\n",
    "# Assuming you have the DataFrame named 'df_eval_short'\n",
    "# Replace 'df_eval_short' with the actual DataFrame name if it's different\n",
    "df_format_gpt4_v1, aa_tmp_list = format_dataframe(df_eval_short, 'gpt-4', 'v1')\n",
    "df_format_gpt4_v2, _ = format_dataframe(df_eval_short, 'gpt-4', 'v2')\n",
    "df_format_gpt3_5_v1, _ = format_dataframe(df_eval_short, 'gpt-3.5-turbo', 'v1')\n",
    "df_format_gpt3_5_v2, _ = format_dataframe(df_eval_short, 'gpt-3.5-turbo', 'v2')\n",
    "\n",
    "# concat and save csv\n",
    "df_format = pd.concat([df_format_gpt4_v1, df_format_gpt4_v2, df_format_gpt3_5_v1, df_format_gpt3_5_v2])\n",
    "df_format.to_csv('./table/all.csv', index=False)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_format)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_error_kind_and_model_and_prompt_ver(df: pd.DataFrame):\n",
    "    df = df.groupby(['error_kind', 'model', 'prompt_ver']).size().reset_index(name='count')\n",
    "    # df = df.sort_values(by=['error_kind', 'model', 'prompt_ver'], key=lambda x: x.map(MODEL_LIST.index))\n",
    "    # filter by USE_MODEL_LIST\n",
    "    df = df[df['model'].isin(USE_MODEL_LIST)]\n",
    "    return df\n",
    "\n",
    "df_eval_groupby = group_by_error_kind_and_model_and_prompt_ver(df_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_table_latex_csv(df: pd.DataFrame, file_name: str):\n",
    "    save_path = './table'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    df.to_csv(os.path.join(save_path, f'{file_name}.csv'), index=False)\n",
    "    df.to_latex(os.path.join(save_path, f'{file_name}.txt'), index=False)\n",
    "    df.to_pickle(os.path.join(save_path, f'{file_name}.pkl'))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def group_by_model_prompt_ver(df_eval: pd.DataFrame):\n",
    "    pass\n",
    "\n",
    "def create_table(df_eval: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    summarize the result as a table\n",
    "\n",
    "    E.g.\n",
    "        Simple table\n",
    "        | model (str) | prompt_ver (str) | success | error | success_rate (only for ada, text-davinci-003)\n",
    "        | --- | --- | --- | --- | --- | --- | --- |\n",
    "        | ada | v1 | 0 | 0 | 0 | 0 | 0 |\n",
    "    \"\"\"\n",
    "    success_rate_models = ['ada', 'text-davinci-003']\n",
    "    # create a table\n",
    "    df_table = pd.DataFrame(columns=['model', 'prompt_ver', 'success', 'error'])\n",
    "    for model in MODEL_LIST:\n",
    "        for prompt_ver in USE_PROMPT_VER:\n",
    "            df = df_eval[(df_eval['model'] == model) & (df_eval['prompt_ver'] == prompt_ver)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "            if model in ['finetuned-davinci', 'chatgpt', 'gpt-4', 'gpt-3.5-turbo', 'code-davinci-002']:\n",
    "                continue\n",
    "            # count success by opentrons_simulate_result == 'ok'\n",
    "            res_count = df['opentrons_simulate_result'].value_counts()\n",
    "            success_count = res_count['ok'] if 'ok' in res_count else 0\n",
    "            print(f'{model}, {prompt_ver}, {success_count}')\n",
    "            error_count = res_count['error'] if 'error' in res_count else 0\n",
    "\n",
    "            if model in success_rate_models:\n",
    "                success_rate = success_count / (success_count + error_count)\n",
    "            else:\n",
    "                success_rate = None\n",
    "\n",
    "            df_table = df_table.append({'model': model, 'prompt_ver': prompt_ver, 'success': success_count, 'error': error_count, 'success_rate': success_rate}, ignore_index=True)\n",
    "\n",
    "    save_table_latex_csv(df_table, 'success_error_count')\n",
    "    return df_table\n",
    "\n",
    "def calculate_success_rate_by_n_turns(df_table: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df_table\n",
    "\n",
    "    success, error, count\n",
    "    13,9,1,gpt-4_1\n",
    "    5,4,2,gpt-4_2\n",
    "    1,3,3,gpt-4_3\n",
    "    1,2,4,gpt-4_4\n",
    "    2,0,5,gpt-4_5\n",
    "\n",
    "    1回めまでの成功率 13/13+9\n",
    "    2回めまでの成功率 13+5/13+9\n",
    "    3回めまでの成功率 13+5+1/13+9\n",
    "\n",
    "    \"\"\"\n",
    "    df_table_success_rate = pd.DataFrame(columns=['model', 'prompt_ver', 'success_by_turn_1', 'success_by_turn_2', 'success_by_turn_3', 'success_by_turn_4', 'success_by_turn_5', 'success_rate'])\n",
    "    conversational_models = ['gpt-4', 'gpt-3.5-turbo']\n",
    "    for model in conversational_models:\n",
    "        for prompt_ver in USE_PROMPT_VER:\n",
    "            # the total num is first call\n",
    "            df_first_call = df_table[(df_table['model'] == f'{model}_1') & (df_table['prompt_ver'] == prompt_ver)]\n",
    "            total = df_first_call['success'].sum() + df_first_call['error'].sum()\n",
    "            previous_success = 0\n",
    "            success_rates = []\n",
    "            for turn in range(1, 6):\n",
    "                model_turn = f'{model}_{turn}'\n",
    "                df = df_table[(df_table['model'] == model_turn) & (df_table['prompt_ver'] == prompt_ver)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                current_success = df['success'].sum()\n",
    "                success_rate = (previous_success + current_success) / total\n",
    "                success_rates.append(success_rate)\n",
    "                previous_success += current_success\n",
    "                print(f'{model_turn}, {prompt_ver}, {success_rate}, {current_success=}, {previous_success=}, {total=}')\n",
    "            new_row = {'model': model, 'prompt_ver': prompt_ver}\n",
    "            for i, success_rate in enumerate(success_rates):\n",
    "                new_row[f'success_by_turn_{i+1}'] = success_rate\n",
    "            df_table_success_rate = df_table_success_rate.append(new_row, ignore_index=True)\n",
    "\n",
    "    # for other models \"ada\", \"text-davinci-003\", simply add success_rate column\n",
    "    for model in MODEL_LIST:\n",
    "        if model not in ['ada', 'text-davinci-003']:\n",
    "            continue\n",
    "        for prompt_ver in USE_PROMPT_VER:\n",
    "            df = df_table[(df_table['model'] == model) & (df_table['prompt_ver'] == prompt_ver)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "            df_table_success_rate = df_table_success_rate.append({'model': model, 'prompt_ver': prompt_ver, 'success_rate': df['success_rate'].values[0]}, ignore_index=True)\n",
    "\n",
    "    save_table_latex_csv(df_table_success_rate, 'success_by_n_turns')\n",
    "    return df_table_success_rate\n",
    "\n",
    "\n",
    "def create_conversation_each_success_table(df_eval: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    | conversation_id (str) | model_group (str) | count (int) | 1st_success (bool) | 2nd_success (bool) | 3rd_success (bool) | 4th_success (bool) | 5th_success (bool) |\n",
    "    | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "    | 2c2cef27-69dc-401c-a8af-01bbb295e342 | gpt-4 | 1 | True | False | False | False | False |\n",
    "    \"\"\"\n",
    "    all_conversation_ids = df_eval['conversation_id'].unique()\n",
    "    df_table = pd.DataFrame(columns=['conversation_id', 'model_group', 'count', 'prompt_ver', '1_success', '2_success', '3_success', '4_success', '5_success'])\n",
    "    for conversation_id in all_conversation_ids:\n",
    "        df_conversation = df_eval[df_eval['conversation_id'] == conversation_id]\n",
    "        for row in df_conversation.itertuples():\n",
    "            count = row.count\n",
    "            if count == 0:\n",
    "                continue\n",
    "            df_table.loc[row.Index, 'prompt_ver'] = row.prompt_ver\n",
    "            df_table.loc[row.Index, 'conversation_id'] = conversation_id\n",
    "            df_table.loc[row.Index, 'model_group'] = row.model_group\n",
    "            df_table.loc[row.Index, 'count'] = count\n",
    "            df_table.loc[row.Index, f'{count}_success'] = True if row.opentrons_simulate_result == 'ok' else False\n",
    "        # df_table.to_csv(f'./result/conversation_each_success_{conversation_id}.csv', index=False)\n",
    "    # all NaN {count}th_success to False\n",
    "    for count in range(1, 6):\n",
    "        df_table[f'{count}_success'] = df_table[f'{count}_success'].fillna(False)\n",
    "\n",
    "    # only left the rows that has biggest count\n",
    "    df_table = df_table.sort_values(by=['conversation_id', 'count'], ascending=False)\n",
    "    df_table = df_table.drop_duplicates(subset=['conversation_id'], keep='first')\n",
    "    df_table = df_table.reset_index(drop=True)\n",
    "\n",
    "    # add 'success_count' column\n",
    "    df_table['success_after_n_call'] = 0\n",
    "    for row in df_table.iterrows():\n",
    "        for count in range(1, 6):\n",
    "            if row[1][f'{count}_success']:\n",
    "                df_table.loc[row[0], 'success_after_n_call'] = count\n",
    "                break\n",
    "\n",
    "    # not conversation model\n",
    "    df_no_conversation = df_eval[df_eval['conversation_id'] == '']\n",
    "    for row in df_no_conversation.itertuples():\n",
    "        new_row = {\n",
    "            'conversation_id': '',\n",
    "            'model_group': row.model_group,\n",
    "            'count': 1,\n",
    "            'prompt_ver': row.prompt_ver,\n",
    "            '1_success': True if row.opentrons_simulate_result == 'ok' else False,\n",
    "            '2_success': False,\n",
    "            '3_success': False,\n",
    "            '4_success': False,\n",
    "            '5_success': False,\n",
    "            'success_after_n_call': 1 if row.opentrons_simulate_result == 'ok' else 0,\n",
    "        }\n",
    "        df_table = df_table.append(new_row, ignore_index=True)\n",
    "\n",
    "    save_table_latex_csv(df_table, 'table_conversation_each_success')\n",
    "    return df_table\n",
    "\n",
    "def create_table_conversation_each(df_eval: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    summarize the result as a table\n",
    "\n",
    "    E.g.\n",
    "        Simple table\n",
    "        | model (str) | prompt_ver (str) | count | success | error | total | success rate | error rate |\n",
    "        | --- | --- | --- | --- | --- | --- | --- |\n",
    "        | gpt-4_1 | v1 | 0 | 0 | 0 | 0 | 0 |\n",
    "        | gpt-4_2 | v1 | 0 | 0 | 0 | 0 | 0 |\n",
    "    \"\"\"\n",
    "    # create a table\n",
    "    df_table = pd.DataFrame(columns=['model', 'prompt_ver', 'success', 'error', 'total', 'success_rate', 'error_rate'])\n",
    "    for model in MODEL_LIST:\n",
    "        for prompt_ver in ['v1', 'v2', 'v3', 'v4', 'v3.1', 'v4.1']:\n",
    "            df = df_eval[(df_eval['model'] == model) & (df_eval['prompt_ver'] == prompt_ver)]\n",
    "            total = len(df)\n",
    "            if total == 0:\n",
    "                print(f'no data for {model}, {prompt_ver}')\n",
    "                continue\n",
    "            success = len(df[df['error_kind'] == 'success'])\n",
    "            error = total - success\n",
    "            success_rate = success / total\n",
    "            error_rate = error / total\n",
    "            if 'gpt-4_' in model or 'gpt-3.5_' in model:\n",
    "                count = model.split('_')[-1]\n",
    "            else:\n",
    "                count = 0\n",
    "            df_table = df_table.append({'model': model, 'prompt_ver': prompt_ver, 'count': count, 'success': success, 'error': error, 'total': total, 'success_rate': success_rate, 'error_rate': error_rate}, ignore_index=True)\n",
    "\n",
    "    return df_table\n",
    "\n",
    "def is_conversation_model(model: str):\n",
    "    if 'gpt-4' in  model or 'gpt-3.5-turbo' in  model:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def create_n_success(df_eval: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    1回目の会話までで成功する確率 (1_success_rate)\n",
    "    2回目の会話までで成功する確率・・・\n",
    "    \"\"\"\n",
    "    df_table = pd.DataFrame(columns=['model', 'prompt_ver', 'count', '1_success_rate', '2_success_rate', '3_success_rate', '4_success_rate', '5_success_rate'])\n",
    "    for model in UNIQUE_MODEL_LIST:\n",
    "        for prompt_ver in ['v1', 'v2', 'v3', 'v4', 'v3.1', 'v4.1']:\n",
    "            df = df_eval[(df_eval['model_group'] == model) & (df_eval['prompt_ver'] == prompt_ver)]\n",
    "            total = len(df)\n",
    "            if total == 0:\n",
    "                print(f'no data for {model}, {prompt_ver}')\n",
    "                continue\n",
    "            if is_conversation_model(model):\n",
    "                count = model.split('_')[-1]\n",
    "            else:\n",
    "                count = 0\n",
    "            new_row = {\n",
    "                'model': model,\n",
    "                'prompt_ver': prompt_ver,\n",
    "                'count': count,\n",
    "            }\n",
    "            previous_success_rate = 0\n",
    "            total = len(df[df['count'] == count])\n",
    "            for n in range(1, 6):\n",
    "                df_n = df[df['success_after_n_call'] >= n]\n",
    "                success_rate = (len(df_n) / total) + previous_success_rate\n",
    "                new_row[f'{n}_success_rate'] = success_rate\n",
    "                previous_success_rate = success_rate\n",
    "            df_table = df_table.append(new_row, ignore_index=True)\n",
    "    return df_table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_eval_count = add_conversation_id(df_eval)\n",
    "df_table_each = create_table_conversation_each(df_eval_count)\n",
    "df_table = create_table(df_eval)\n",
    "df_table_success_rate = calculate_success_rate_by_n_turns(df_table)\n",
    "\n",
    "\n",
    "df_table_each_success = create_conversation_each_success_table(df_eval_count)\n",
    "\n",
    "# df_table_each_success_rate = create_n_success(df_table_each_success)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40f42281c7c4c96fcfb6760bfacdc23ca735e3f7cc24281f17102637017dc3ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
